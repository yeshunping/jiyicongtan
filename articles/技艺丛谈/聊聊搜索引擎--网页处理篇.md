前面的一些文章发得有点聪明，主要是想在大家过节前发一篇，别浪费了公众号的发送次数。上文谈的是爬虫部分，《聊聊搜索引擎--爬虫篇》。文章里提到之前写过两篇有关爬虫的文章，忘了链接过去了，这里放一下，方便有兴趣的读者阅读。《不想浪费生命？那就掌握这些搜索技巧吧》，《网页爬虫分享大纲，以及一份常见爬虫问题列表》

前文提到，搜索引擎一般可以分为爬虫模块，网页处理，索引模块，检索模块，排序模块几个模块。爬虫负责抓取数据，算是数据生产方。今天我们要说的是数据的加工部门。也就是网页处理。

网页处理在大部分介绍里，都算在爬虫部分。我呆过的第一家公司，并没有网页处理相关的小组。我当时基本上算是最主要的网页处理工程师了，写了包括网页解析基础库，各种网页里的质量信号的抽取等。不过我呆过的第二家搜索公司，当时有一个小组是专门做网页解析的。当然，本文要谈的不局限于网页解析，而是在索引之前的搜索特征提取。在我做搜索的时候，机器学习还没那么火，我们一般叫质量信号之类的。但是这些质量信号，主要是服务排序的，排序作为一个机器学习任务，使用到的因素，我们称之为特征比较好理解。而网页解析等大部分工作，可以看成网页排序这个复杂Task的特征工程。

## Index pipeline

在一个网页搜索中，完整的index pipeline 很复杂，涉及到死链检测，soft 404页面检测，标题抽取，正文抽取，网页发布时间抽取，结构化信息抽取（比如论坛的各个帖子的内容，作者，时间等。再比如电影的导演，主演，名字，别名等），host rank 计算，page rank计算，色情页面检测，垃圾页面检测，作弊网页检测，重复页面检测，病毒页面检测等等。

## Html Parser

那么要完成这么多的搜索排序的特征工程，首先我们需要打造一个方便易用的网页解析库。一般我们会实现一个html parser, 基本DomTree 或者是SAX。编写一个这样的引擎并不容易。解析一个网页比较容易，但是要能够解析几千亿的网页而不crash，并且碰到网页很大（比如超过2M），或者网页很病态（比如只有open tag, 没有close tag），解析的速度还能够很快，并且内存依然不会爆掉，需要对这个网页解析库进行千锤百炼。当然，网上已经有了不少开源的解析库了。不过大部分情况是这样的，要不接口不友好，要不性能糟糕，要么代码风格不好。之前谷歌也开源了一个网页解析的项目，我们测试下来，性能实在是跟不上，不太适合搜索引擎。

## Xpath

网页解析除了html parser ，一般还需要方便工程师进行网页信息抽取，最好是基于配置文件进行抽取，因此一般需要支持xpath 查询语言。Xpath 的资料网上很丰富，大家有兴趣可以结合XML语言去学习了解。那如何高效支持 Xpath 进行信息抽取，甚至基于整个配置文件来抽取某一类网页，甚至某一类网站的多种信息抽取，就又有更高level的需求了，比如是否实现一个template config parser 之类的解析库。 核心诉求是，配置规则很简单，方便易学，甚至可以找一些三四千块钱的编辑就可以进行配置。那么怎么把这个库实现得优雅高效，方便易学，甚至如何开发出一些配套的浏览器工具，便是这块的核心竞争力。据我所知，百度等搜索公司对这方面都花了不少研发精力。

网页解析这里有一个核心竞争力在于，如何利用你的工程能力，对网页解析库这样底层的基础工具进行性能优化，比如优化个10-20%，或者优化了三五倍的性能出来，那整体的搜索引擎相关的计算集群资源，就能得到很大的改进，省出来的银子，不见得比索引性能优化，或者是检索性能优化，或者是各种机器学习模型的排序性能优化来得少。

## CSS Parser

网页解析还有好几个有挑战的事情，比如要解析出来网页里字体的大小（字体很小也是作弊的手段之一），颜色（颜色搞成和背景色一样，也是作弊的手段之一），或者是文字是否可见（常规的作弊手段之一）等，都是需要比较高阶的解析的。如果学过网页设计，大家应该知道这块需要对CSS进行解析。CSS一般分为tag 内部的，html 文件内的，外部CSS文件等。那么如何高效地进行CSS文件，并与HTML解析结果进行结合，就有不少工作值得深入。

此外，CSS文件的压缩和存储/读取，也有一定的挑战。原因在于，不像html 这样，大部分网页的内容差异都比较大，CSS 一般是给一个网站，或者是一类风格的网页设计的，文件的规模比较少。有一些建站工具做出来的，甚至大部分CSS都一样。既然CSS文件的内容有特殊的规律，那么针对CSS的压缩，可能就会存在特殊的算法。还有一个问题是，网页的内容可能不会变，但是网页的风格却可能会被更新。在网页解析的时候，往往一次性分析百亿级别的网页，那如何高效读取各个网页对应的CSS文件呢？实时抓取么？还是存在mysql ? Nosql ? 还是怎么去特殊处理？

CSS 文件怎么解析呢？ 也有不少开源的项目，当然也可以参考各大浏览器开源项目。不过浏览器项目里一般代码依赖比较复杂，很难单独抽取出来。

## Javascript

其实这部分放在爬虫部分更合适。在抓取的时候，往往会碰到页面跳转，或者是有一部分关键的网页内容，是在页面本身加载完毕后，才开始动态加载的。比如做新闻的Hub页抓取的时候，往往会发现我们想要的新闻内容页的链接信息，是通过 js 来生成的。那怎么办？如果只是抓取一个网站，当然我们可以通过抓包分析，靠人工来总结浏览器背后的网络行为，然后写代码模拟浏览器的行为。通用点的，公司内搭建或者开发一套浏览器抓取的方案，特殊的网页，都使用这些特殊的抓取服务来抓，这样在下游做网页解析的时候，看到的 html 都是一样带有我们想要抽取的页面内容的了。

之所以放在这部分，很简单，js 是一门脚本语言，脚本语言的执行，其实也可以kan c


一些相关资料

[http://research.google.com/people/jeff/WSDM09-keynote.pdf](http://research.google.com/people/jeff/WSDM09-keynote.pdf)

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTMzNDk0NjI2M119
-->